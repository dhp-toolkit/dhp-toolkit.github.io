{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76166c62-9f0b-4fa5-9681-ff7937a76344",
   "metadata": {},
   "source": [
    "## Named Entity Recognition With SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c398a2-4c34-408b-a031-da6c1fe99872",
   "metadata": {},
   "source": [
    "In this tutorial we will learn how to use **spaCy** to identify place names and people's names from the historical document `AgreementWithTrucialChiefs.txt`.\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "\n",
    "First, you'll need to install **spaCy** and download an English language model. To install them, run the below commands in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22466af1-d4ce-404d-b367-62e3f2ff594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad53d8a-6f44-49d5-9c9d-3c11c6f2358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287ae5d-dc62-4e49-95c2-8eeb947c1c8f",
   "metadata": {},
   "source": [
    "### Step 2: Set Up and Load SpaCy Model\n",
    "\n",
    "We need to import the necessary libraries and load the pre-trained `spaCy` English language model. This model will be used to identify entities in your text. We'll also import `re` for regular expression operations needed in text cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df4091-00bf-4829-a232-8a98c8e0a15e",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- `import spacy`: Imports the main spaCy library.\n",
    "- `import re`: Imports the regular expression module, which is very useful for flexible text manipulation.\n",
    "- `nlp = spacy.load(\"en_core_web_sm\")`: Loads the small English language model. This model includes components for tokenization, POS tagging, dependency parsing, and, importantly, named entity recognition.\n",
    "- The `try-except` block ensures that if the model isn't found (meaning it wasn't downloaded or is in an inaccessible location), the notebook provides a helpful message instead of crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abaefc50-687e-444c-ad0a-9d61904c367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re # For regular expressions, useful for text cleaning\n",
    "\n",
    "# Load the small English language model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"Error: Language model 'en_core_web_sm' not found.\")\n",
    "    # If the model isn't found, we can't proceed, so we'll exit gracefully.\n",
    "    raise SystemExit(\"SpaCy model not found. Please download it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56294d5e-bdc1-4c63-b654-98afcfdf9a43",
   "metadata": {},
   "source": [
    "### Step 3: Define Text Cleaning Function\n",
    "\n",
    "Before performing NER, it's a good practice to clean your raw text. However, for NER, \"cleaning\" should be conservative. We want to remove noise (like OCR errors or special tags) without removing information crucial for NER, such as capitalization (which helps identify proper nouns) or sentence structure.\n",
    "\n",
    "The `clean_text_for_ner()` function will:\n",
    "- Remove specific known artifacts like \"[unclear]\" tags.\n",
    "- Normalize whitespace by replacing multiple spaces, tabs, and newlines with a single space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591aacd-0339-415f-8469-a23560909ea0",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- The function takes `text_content` as input.\n",
    "- It uses `replace()` to target specific strings that are noise.\n",
    "- `re.sub(r'\\s+', ' ', cleaned_text).strip()` is a regular expression that finds one or more whitespace characters (`\\s+`) and replaces them with a single space.\n",
    "- `.strip()` removes leading/trailing whitespace. This ensures consistent spacing.\n",
    "\n",
    "**Important**: Avoid aggressive cleaning for NER, such as:\n",
    "- Lowercasing (NER relies on capitalization)\n",
    "- Removing all punctuation (NER relies on sentence boundaries)\n",
    "- Removing stopwords (context is important for NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd4a3586-0b81-4279-9ca5-1d4dbe28aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_ner(text_content):\n",
    "    \"\"\"\n",
    "    Performs conservative text cleaning suitable for NER.\n",
    "    Focuses on removing noise without losing crucial information like capitalization.\n",
    "    \"\"\"\n",
    "    # Replace common OCR artifacts or specific unwanted tags (in the case of our document, tags like \"[unclear]\")\n",
    "    cleaned_text = text_content.replace(\"[unclear]\", \" \")\n",
    "\n",
    "    # Normalize whitespace: replace multiple spaces/newlines with a single space\n",
    "    # This helps spaCy's tokenizer work more consistently\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb1a11-dfe7-4f78-9c58-f56f1e936b32",
   "metadata": {},
   "source": [
    "### Step 4: Define NER Execution Function\n",
    "\n",
    "The `perform_ner()` function below will take the cleaned text and the loaded spaCy model, then run the NER process. It will extract all identified PERSON (people's names) and GPE (geopolitical entity) / LOC (location) entities. These lists will be \"raw\" at this stage, meaning they might still contain some false positives before post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043986a6-f89f-4b37-a61d-5b551af25a59",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "- `doc = nlp_model(text_to_process)`: This is where spaCy processes the text and identifies entities.\n",
    "- `doc.ents`: This is a list of all identified entities in the doc object.\n",
    "- The loop iterates through these entities, checking their `label_` (e.g., \"PERSON\", \"GPE\", \"LOC\") and appends the entity's text (`ent.text`) to the appropriate raw list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd2544f3-0dff-47a1-ab16-47ddee4ce819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner(text_to_process, nlp_model):\n",
    "    \"\"\"\n",
    "    Runs spaCy's NER on the given text and extracts raw PERSON, GPE, and LOC entities.\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text_to_process)\n",
    "\n",
    "    raw_people_names = []\n",
    "    raw_place_names = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            raw_people_names.append(ent.text)\n",
    "        elif ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "            raw_place_names.append(ent.text)\n",
    "\n",
    "    return raw_people_names, raw_place_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695efcc2-8ef7-4e99-93d5-bb84157e3bb0",
   "metadata": {},
   "source": [
    "### Step 5: Run NER\n",
    "\n",
    "Let's now put all the functions together: read the file, clean the text, perform NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c60299e-de51-4d1d-9aec-c74aff9ddb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 'AgreementWithTrucialChiefs.txt'. Length: 16369 characters.\n",
      "\n",
      "--- Performing Text Cleaning ---\n",
      "\n",
      "--- Performing Named Entity Recognition ---\n",
      "Raw PERSON entities found: 54\n",
      "Raw GPE/LOC entities found: 35\n",
      "\n",
      "Identified People's Names (Raw):\n",
      "- Simla\n",
      "- Govern-\n",
      "- C. J. E.\n",
      "- C. E.\n",
      "- Zaid bin Khalifah\n",
      "- Homaid bin Rashid\n",
      "- Shaikh Ahmed bin Abdullah\n",
      "- Homaid bin Abdullah\n",
      "- --,,--Rashid bin Muktoom\n",
      "- Sakar bin Khalid\n",
      "- Sheikh\n",
      "- W J.Cunningham\n",
      "- Simla Dy Secy\n",
      "- W.H. 3/6/92\n",
      "- Hamad bin Abdellah\n",
      "- Ras al Khaimah\n",
      "- A. C. Talbot\n",
      "- C. I. E.\n",
      "- A.C. Tabot\n",
      "- L. S. True Copy\n",
      "- Ahmad bin Abdullah\n",
      "- Umm al Kawain\n",
      "- A.C. Talbot\n",
      "- Persian Gulf\n",
      "- Dated Umm-ul Kawain\n",
      "- L. S. True Copy JHBaille Asst\n",
      "- Hameid bin Rashid\n",
      "- A.C. Talbot C. I. E. Political Resident\n",
      "- Dated Ajman\n",
      "- Shaaban\n",
      "- L. S.\n",
      "- JUBauille Asst\n",
      "- Saggar bin Khalid\n",
      "- Dated Sharjah\n",
      "- Talbot Resident Persian Gulf\n",
      "- Zaud bin Khalifah\n",
      "- A.C. Talbot\n",
      "- C. I. E.\n",
      "- Dated Abu Dhabi\n",
      "- Hijri\n",
      "- Copy H. Baille Asst\n",
      "- bin Ali\n",
      "- L. S. True\n",
      "- Ross\n",
      "- bin Ali\n",
      "- Sajar bin Khalid\n",
      "- Zayed bin Khalifah\n",
      "- Hamid bin Abdullah\n",
      "- Rasal Khaimah\n",
      "- Ahmad bin Abdullah\n",
      "- bin Maktum\n",
      "- Ahmed bin Rashid\n",
      "- Hart\n",
      "- Talbot Poll\n",
      "\n",
      "Identified Place Names (Raw):\n",
      "- Bushire\n",
      "- the Arab Coast\n",
      "- England\n",
      "- Shargah\n",
      "- the Arab Coast\n",
      "- M.Chapuy\n",
      "- Persian Gulf\n",
      "- Persian Gulf\n",
      "- the Persian Gulf\n",
      "- Secry\n",
      "- Shargah\n",
      "- Bushire\n",
      "- C.E.\n",
      "- Bahrain\n",
      "- Coast\n",
      "- Ratification\n",
      "- India\n",
      "- Bahrain\n",
      "- Govt\n",
      "- India\n",
      "- Lieut\n",
      "- Lieut\n",
      "- Persian Gulf\n",
      "- Lieut\n",
      "- Persian Gulf\n",
      "- Persian Gulf\n",
      "- Dubai\n",
      "- Talbot\n",
      "- Persian Gulf\n",
      "- Bahrain\n",
      "- Lieut\n",
      "- Persian Gulf\n",
      "- Fort\n",
      "- Bahrain\n",
      "- P.Gulf\n"
     ]
    }
   ],
   "source": [
    "file_name = \"AgreementWithTrucialChiefs.txt\" # Ensure this file is in the same directory as your notebook\n",
    "text_content = \"\" # Initialize to empty string\n",
    "\n",
    "try:\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        text_content = file.read()\n",
    "    print(f\"Successfully read '{file_name}'. Length: {len(text_content)} characters.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_name}' was not found. Please check the file name and path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")\n",
    "\n",
    "if text_content: # Only proceed if text_content was successfully loaded\n",
    "    # 2. Perform Text Cleaning\n",
    "    print(\"\\n--- Performing Text Cleaning ---\")\n",
    "    cleaned_text = clean_text_for_ner(text_content)\n",
    "    # Uncomment the line below to see a snippet of the cleaned text if you're curious:\n",
    "    # print(f\"Cleaned text snippet (first 500 chars): {cleaned_text[:500]}...\")\n",
    "\n",
    "    # 3. Perform NER on the cleaned text\n",
    "    print(\"\\n--- Performing Named Entity Recognition ---\")\n",
    "    raw_people, raw_places = perform_ner(cleaned_text, nlp)\n",
    "    print(f\"Raw PERSON entities found: {len(raw_people)}\")\n",
    "    print(f\"Raw GPE/LOC entities found: {len(raw_places)}\")\n",
    "\n",
    "    # 4. Display Results\n",
    "    print(\"\\nIdentified People's Names (Raw):\")\n",
    "    if raw_people: # Check if the list is not empty after filtering\n",
    "        for name in raw_people:\n",
    "            print(f\"- {name}\")\n",
    "    else:\n",
    "        print(\"No people names identified.\")\n",
    "\n",
    "    print(\"\\nIdentified Place Names (Raw):\")\n",
    "    if raw_places: # Check if the list is not empty after filtering\n",
    "        for place in raw_places:\n",
    "            print(f\"- {place}\")\n",
    "    else:\n",
    "        print(\"No place names identified.\")\n",
    "else:\n",
    "    print(\"No content to process. Please ensure the file is correctly read.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6251e-3310-49db-81ba-31a450d4563e",
   "metadata": {},
   "source": [
    "### Step 6: Do Post-Processing for NER Output\n",
    "\n",
    "You can see from the above result that spaCy's English Language model identified some entities incorrectly. NER models can sometimes make mistakes or pick up non-names (like titles, abbreviations, or common words), especially when working with historical documents. That's why it's important to do post-processing to refine the results. \n",
    "\n",
    "The `post_process_ner_output()` function below will filter the raw entities based on observed patterns of incorrect classifications. This step is highly customizable based on the specific noise you find in your document type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b911d4e-2f9d-42fc-9891-738212313563",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "- `exclude_patterns`: These lists are populated with the terms we've observed to be misclassified. They are defined separately for PERSON and PLACE types.\n",
    "- `min_length`: Helps filter out very short, often meaningless, extractions.\n",
    "- `ent.replace('\\n', ' ').strip()` ensures consistent formatting of entities, even if they span multiple lines.\n",
    "\n",
    "**Filtering Logic:**\n",
    "  \n",
    "- `any(excl_pat.lower() in normalized_ent.lower() for excl_pat in exclude_patterns)`: Checks if any of the unwanted patterns are contained within the extracted entity. This is robust for variations.\n",
    "- `any(char.isdigit() for char in normalized_ent)`: Removes entries containing numbers (e.g., \"No: 167\", dates).\n",
    "- Length check `len(normalized_ent) < min_length`: Filters out very short strings.\n",
    "- Specific person-related cleanups (like handling \"C.I.E.\" within a name or removing trailing punctuation) are included.\n",
    "  \n",
    "This function needs iterative refinement. As you review your output, if you see new types of incorrect entries, you can update the `exclude_patterns` lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a935eb16-3ae9-4d46-8cbe-d52455e9e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_ner_output(entities_list, entity_type):\n",
    "    \"\"\"\n",
    "    Filters out common false positives from the NER output based on observed patterns.\n",
    "    This list is flexible and should be refined iteratively after reviewing initial raw NER results.\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "\n",
    "    # Define exclusion patterns based on previous observations from your document\n",
    "    # You can expand these lists as you find more \"noise\" in your output.\n",
    "    if entity_type == \"PERSON\":\n",
    "        exclude_patterns = [\n",
    "            # Titles, honors, roles, and common abbreviations seen in raw output\n",
    "            \"C. E.\", \"C. I. E.\", \"C. J. E.\", \"L. S.\", \"L. S. True Copy\", \"Sheikh\",\n",
    "            \"L. S. True Copy JHBaille Asst\", \"L. S. True\", \"JUBauille Asst\", \"Copy H. Baille Asst\",\n",
    "            \"A.C. Talbot C. I. E. Political Resident\",\n",
    "\n",
    "            # Partial words or non-name phrases seen in raw output\n",
    "            \"Govern-\", \"Dated Umm-ul Kawain\", \"Dated Ajman\", \"Dated Sharjah\", \"Dated Abu Dhabi\",\n",
    "\n",
    "            # Time/Date related terms seen in raw output\n",
    "            \"Shaaban\", \"Hijri\",\n",
    "\n",
    "            # Filter this misspelling/variant\n",
    "            \"A.C. Tabot\", \"A. C. Talbot\",\n",
    "\n",
    "            # Specific places mistakenly identified as people \n",
    "            \"Persian Gulf\", \"Umm-ul Kawain\", \"the Persian Gulf\", \"Rasal Khaimah\", \"Ras al Khaimah\", \"Umm al Kawain\"\n",
    "        ]\n",
    "        min_length = 2 # Minimum length for a person's name to be considered valid\n",
    "\n",
    "    \n",
    "    elif entity_type == \"PLACE\":\n",
    "        exclude_patterns = [\n",
    "            \"Return\", \"C.E.\", \"P.Gulf\", \"Foreign Dept\", \"Deptt\", \"Govt\",  \"Secry\", \"Ratification\", \"Fort\",\n",
    "            \"Lieut\", \"M.Chapuy\", \"Talbot\", \"the Persian Gulf\"\n",
    "        ]\n",
    "        min_length = 2 \n",
    "\n",
    "    for ent in entities_list:\n",
    "        # Normalize the entity text: replace newlines with spaces and strip extra whitespace\n",
    "        normalized_ent = ent.replace('\\n', ' ').strip()\n",
    "\n",
    "        # Filter out if it matches any exclusion pattern (case-insensitive check)\n",
    "        if any(excl_pat.lower() in normalized_ent.lower() for excl_pat in exclude_patterns):\n",
    "            continue\n",
    "\n",
    "        # Filter out if it contains digits (often indicates dates or IDs)\n",
    "        if any(char.isdigit() for char in normalized_ent):\n",
    "            continue\n",
    "\n",
    "        # Filter out very short entities based on defined minimum length\n",
    "        if len(normalized_ent) < min_length:\n",
    "            continue\n",
    "        \n",
    "        # Additional specific filters or cleaning for PERSON entities\n",
    "        if entity_type == \"PERSON\":\n",
    "            # Remove trailing non-alphanumeric/non-space characters\n",
    "            normalized_ent = re.sub(r'[\\s.,-]+$', '', normalized_ent)\n",
    "            # Remove leading numbers/symbols that might be OCR errors\n",
    "            normalized_ent = re.sub(r'^[^\\w\\s]+', '', normalized_ent).strip()\n",
    "\n",
    "\n",
    "        # If all filters pass, add to the filtered list\n",
    "        filtered.append(normalized_ent)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ebfde91-9c09-440c-b80e-cc03ba47d060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Post-processing NER Output ---\n",
      "\n",
      "Identified People's Names (Filtered):\n",
      "- A.C. Talbot\n",
      "- Ahmad bin Abdullah\n",
      "- Ahmed bin Rashid\n",
      "- Hamad bin Abdellah\n",
      "- Hameid bin Rashid\n",
      "- Hamid bin Abdullah\n",
      "- Hart\n",
      "- Homaid bin Abdullah\n",
      "- Homaid bin Rashid\n",
      "- Rashid bin Muktoom\n",
      "- Ross\n",
      "- Saggar bin Khalid\n",
      "- Sajar bin Khalid\n",
      "- Sakar bin Khalid\n",
      "- Shaikh Ahmed bin Abdullah\n",
      "- Simla\n",
      "- Simla Dy Secy\n",
      "- Talbot Poll\n",
      "- W J.Cunningham\n",
      "- Zaid bin Khalifah\n",
      "- Zaud bin Khalifah\n",
      "- Zayed bin Khalifah\n",
      "- bin Ali\n",
      "- bin Maktum\n",
      "\n",
      "Identified Place Names (Filtered):\n",
      "- Bahrain\n",
      "- Bushire\n",
      "- Coast\n",
      "- Dubai\n",
      "- England\n",
      "- India\n",
      "- Persian Gulf\n",
      "- Shargah\n",
      "- the Arab Coast\n"
     ]
    }
   ],
   "source": [
    "# Post-process the NER output\n",
    "print(\"\\n--- Post-processing NER Output ---\")\n",
    "# Apply filtering, then convert to a set to remove duplicates, then sort\n",
    "final_people_names = sorted(list(set(post_process_ner_output(raw_people, \"PERSON\"))))\n",
    "final_place_names = sorted(list(set(post_process_ner_output(raw_places, \"PLACE\"))))\n",
    "\n",
    "# Display Results\n",
    "print(\"\\nIdentified People's Names (Filtered):\")\n",
    "if final_people_names: # Check if the list is not empty after filtering\n",
    "    for name in final_people_names:\n",
    "        print(f\"- {name}\")\n",
    "else:\n",
    "    print(\"No people names identified after filtering.\")\n",
    "\n",
    "print(\"\\nIdentified Place Names (Filtered):\")\n",
    "if final_place_names: # Check if the list is not empty after filtering\n",
    "    for place in final_place_names:\n",
    "        print(f\"- {place}\")\n",
    "else:\n",
    "    print(\"No place names identified after filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976adc19-3392-47ea-b26e-d121ce9aa0bf",
   "metadata": {},
   "source": [
    "### Step 7: Saved Output as TXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49d241-fb69-4309-8d08-af66aa0a1dbf",
   "metadata": {},
   "source": [
    "Below is the code to save the identified people and place names as TXT files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5eed0d2-6eb0-4c59-815c-2f345a0951b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified people names saved to 'people_names.txt'\n",
      "Identified place names saved to 'place_names.txt'\n"
     ]
    }
   ],
   "source": [
    "# Save identified people's names to a text file\n",
    "output_file_people = \"people_names.txt\"\n",
    "with open(output_file_people, 'w', encoding='utf-8') as f:\n",
    "    for name in final_people_names:\n",
    "        f.write(name + '\\n')\n",
    "print(f\"Identified people names saved to '{output_file_people}'\")\n",
    "\n",
    "# Save identified place names to a text file\n",
    "output_file_places = \"place_names.txt\"\n",
    "with open(output_file_places, 'w', encoding='utf-8') as f:\n",
    "    for place in final_place_names:\n",
    "        f.write(place + '\\n')\n",
    "print(f\"Identified place names saved to '{output_file_places}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18742614-59fb-458d-8fb1-42739a21e775",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Steps you can take for further improvement:\n",
    "\n",
    "- **Review Remaining Errors**: Carefully examine any remaining incorrect entries in the \"Cleaned and Filtered\" lists. Add new patterns to the exclude_patterns in post_process_ner_output for these specific cases.\n",
    "- **Consider Larger SpaCy Models**: If accuracy is still a concern, en_core_web_lg (large) or en_core_web_trf (transformer-based) models from spaCy offer higher accuracy but require more memory and computational resources. You would install them using python -m spacy download en_core_web_lg.\n",
    "- **Custom Rule-Based Matching**: For very specific patterns that spaCy's statistical model misses (e.g., consistently formatted IDs or very specific types of titles), you could use spaCy's Matcher or EntityRuler components.\n",
    "- **Custom Model Training**: For truly high accuracy on highly specialized texts, the best approach is to create a custom dataset by manually annotating a portion of your documents and then training a spaCy NER model on it. This is a more advanced task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddf68b-50cd-479b-a002-c00c3ee6d7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
